
### 1. 线性变换

- **设** ：$A \in \mathbb{R}^{m \times n},x \in \mathbb{R}^n$
	- $A$ 可以看成是 $n$ - 维 列向量 $A = [a_1,...,a_n],a_i \in \mathbb{R}^m$
	- $x = (a_1,...,a_n)^T$
- **线性变换**：$Ax = b = [b_1,...,b_n] = x_1a_1 + ... + x_na_n$
	- **列空间**：$Col(A)$ - 是由 $A$ 的列向量 $a_1,...,a_n$ 张成的空间
		- 假设 $m > n$
		- 如果 $dim(Col(A)) = n$，那么列向量张成了 $\mathbb{R}^m$ 空间的一个 **$n$ - 维子空间**
		- 如果 $dim(Col(A)) = m$，那么列向量张成了整个 $\mathbb{R}^m$ 空间
	- **线性组合**：$Ax = x_1a_1 + ... + x_na_n$ 是 $A$ 的列向量的线性组合，$Ax \in Col(A)$
	- **空间变换**：$A \in \mathbb{R}^{m \times n}$ 定义了由 $\mathbb{R}^n$ 空间到 $\mathbb{R}^m$ 空间的一个变换（线性映射）
		- $x \in \mathbb{R}^n \mapsto Ax \in \mathbb{R}^m$
	- **线性独立**：如果 $a_1,...,a_n$ 线性无关（**线性独立**），它们构成了 $Col(A)$ 中的**一组基**
		- $rank(A) = n$
	- **基变换和向量变换**：
		- **主动视角**：参考系没有变化，向量 $x$ 在算子 $A$ 的作用下拉伸变换得到了新的向量 $b$
		- **被动视角**：向量没有变化，在新基 $A$ 下，向量坐标发生了变化 $A^{-1}x$ 是新基下的坐标
			- 如果 $A$ 是特征向量构成的矩阵，$A^{-1}x$ 得到了向量 $x$ 在特征向量基下的坐标
- **行列式**：
	- $det(A) \neq 0, A \in \mathbb{R}^{n \times n}$ - $A$ 的**列向量线性无关**，$A$ **可逆**
	- $|det(A)|$ - 体积的**缩放倍数**，$det(A)$ - 符号决定是否翻转

### 2. 特征值和特征向量

- **欧几里得范数**：
	- **非负性** ： $||v|| \geq 0$
	- **正定性**：$||v|| = 0$ 仅当 $v = 0$
	- **齐次性**：$||cv|| = |c| \cdot ||v||$
- **特征向量**：
	- 设 $A \in R^{n \times n}$，存在一个非零向量 $x \ne 0 \in R^n$  称为 $A$  的特征向量，使得  $Ax = \lambda x$
- **特征空间**：矩阵 $A \in R^{n \times n}$ 的特征空间 $E_{\lambda}$ 定义为所有特征向量（以及零向量） 的集合
	- $E_{\lambda} = \{x : Ax = \lambda x\}$
	- 一个特征值可以对应多个特征向量，一个特征向量对应一个特征值
	- $E_{\lambda}$ 是 $R^{n}$ 的一个子空间，满足：包含零向量，加法封闭性，数乘封闭性
- **特征方程**：$(A - \lambda I_n)x = 0$
	- 方程有**平凡解** ===> $det(A - \lambda I_n) = 0$
- **特征多项式**：$p(\lambda)=c_n​\lambda^n+c_{n−1}​\lambda^{n−1}+⋯+c_1​\lambda+c_0$
- **矩阵对角化条件**：$A \in \mathbb{R}^{n \times n}$
	- 有 $n$ 个不同的特征值 $\lambda_1,...,\lambda_n$
	- **对称矩阵**
	- 存在 n 个**线性无关** 的 **特征向量**
	- 存在一个**可逆矩阵** **P** 和一个 **对角矩阵** Λ，使得 $P^{-1}AP = Λ$
	- $\lambda_1,...\lambda_r$ 是其不同的特征值，A 可对角化当且仅当存在一个由特征向量构成的基，等价于：$dim(E_{\lambda_1}) + ... + dim(E_{\lambda_r}) = n$
- $det(A) = det(P Λ P^{-1}) = det(P)det(Λ)\frac{1}{det(P)}=det(Λ)$
- 设 $A = PΛP^{-1}, k \in \mathbb{N}, A^k = PΛ^kP^{-1}$
- **几何重数**：如果一个特征空间 $E_{\lambda}$ 的维数是 $k$，那么对应的特征值在矩阵 Λ 中出现 k 次。我们也称数字 k 为
- **求 $Ax$ 在特征基下的坐标：**
	- B 是 A 的特征向量构成的矩阵 - 特征基
	- $x$ 在特征基下的坐标： $[x]_B = B^{-1}x$
	- $A$ 在特征基下的矩阵：$[A]_B = B^{-1}AB = P$ - 如果矩阵可以对角化，P是对角矩阵
	- $Ax$ 在特征基下的坐标：$[Ax]_B = [A]_B [x]_B = B^{-1}AB[x]_B$

### 3. 正交和投影

- **正交矩阵的逆**：$U^{-1} = U^{T}$


### 4. 二次型

- **我们的研究对象**：形如 $ax^2 + bxy + cy^2 + dx + ey + c = 0$ 的函数
	- 只有二次项，例如 $ax^2$ 或者 $bxy$ 会影响函数图像
	- 只含有 **二次项的函数** 我们称之为 **二次函数**
- **二次型的矩阵写法**：$\vec{x}^T \cdot A \cdot \vec{x}$
	- 我们通过微积分方式研究曲线（面）可以通过研究矩阵的方式进行，这样在大量参数的场景下（例如 **几千亿** 个神经网络参数）会更方便
	- **例如**：$f(x,y) = ax^2 + 2bxy + cy^2$
	- **矩阵改写**：
	$$\vec{x} = \begin{bmatrix} x \\ y \end{bmatrix}, \quad A = \begin{bmatrix} a & b \\b & c \end{bmatrix}$$
	$$f(x,y) = \begin{bmatrix} x & y \end{bmatrix} \cdot \begin{bmatrix} a & b \\b & c \end{bmatrix} \cdot \begin{bmatrix} x \\ y \end{bmatrix}$$
	- **注意**：为了方便处理，我们把交叉项的系数拆为 $2b$ 的形式，例如 $3xy$ 我们拆为 $2(\frac{3}{2})xy$
	- **对称矩阵**：不难看出，以上的矩阵改写方式，矩阵 $A$ 称为了 **对称矩阵** 的形式，对称矩阵在今后的应用中有很多好处，包括：其特征值是实数，图形上只有拉伸而没有旋转（虚特征值会导致图形旋转）
	- $n$ **维推广**：
	$$\vec{x} = \begin{bmatrix} x_1 & x_2 & ... & x_n \\ \end{bmatrix}, \quad A = \begin{bmatrix} A_{11} & A_{12} & ... & A_{1n} \\ ... & A_{22} & ... & ... \\ ... & ... & ... & ... \\ ... & ... & ... & ... \\ A_{n1} & ... & ... & A_{nn}\end{bmatrix}$$
$$f(x_1,x_2,...,x_n) = A_{11}x_1^2 + A_{22}x_2^2 + ... + A_{nn}x_n^2 + (A_{12}A_{21})x_1x_2 + (A_{34}A_{43})x_3x_4 + ...$$

此处，如果我们采用 **对称矩阵** 形式，交叉项 $A_{12} = A_{21}$，以上通项可以写为 $2A_{12}x_1x_2$

- **二次型定义及性质**：

	- **因为**：$Q(x,y) = \vec{x}^T \cdot A \cdot \vec{x}$
		- $A\vec{x}$ - 向量线性变换，**结果是一个向量**
		- $\vec{x}^T \cdot A\vec{x}$ - 向量的转置和向量相乘，是一个 “**内积**”，结果是一个**实数**
	- **所以**：$R^n$ 上的 **二次型** 是 **一个函数** ，由 $Q(x) = x^T Ax$ 给出，其中 $A \in R^{n \times n}$ 
	- **特性**：二次型 $Q(x) > 0$，说明 $\vec{x}^T$ 和 $A\vec{x}$ 两个向量夹角是锐角，即矩阵 $A$ 作用于向量 $\vec{x}$ 后（线性变换）和原始向量 “**同向**”。同理，$Q(x) < 0$，说明夹角是钝角，也就是 “**反向**”，$Q(x) = 0$，说明是直角，也就是正交变换。
		- **内积的几何公式**：$\vec{a} \cdot \vec{b} = ||\vec{a}|| \cdot ||\vec{b}|| \cdot cos\theta$
		- $cos$ 函数在 $(0, \frac{\pi}{2})$ 大于 0, $(\frac{\pi}{2},\pi)$ 小于 0
	- **正定性**：
		- $\ge 0$ ：正定
		- $> 0$ ：半正定
		- $< 0$ ：负定
		- $\leq 0$ ：半负定
		- 不确定符号：不定
	- **最大值与最小值**：
		- 正定：**开口朝上，有全局极小值**
		- 负定：**开口朝下，有全局极大值**
		- 半正定：**开口朝上，有极小值，但不是严格极小值**
		- 半负定：**开口朝下，有极大值，但不是严格极大值**
		- 不定：**既没有极小值也没有极大值**

- **正定性的判定准则**：**特征值** $\lambda_1,\lambda_2,...$（矩阵的对角线）是否全为正数
- **主轴定理**：$A \in ℝ^{n \times n}$ 是 一个 **实对称矩阵**，通过 P **正交地对角化** 为矩阵 $Λ = diag(λ₁,...,λₙ)$，那么对于 $y = P^T x$，有：$x^T Ax = y^T Λy = \Sigma_{i=1}^n \lambda_i \cdot y_i^2$
	- **谱定理**（Spectral Theorem）：**实对称矩阵** **必定** 可以正交对角化（这也就是上面构建二次型矩阵用对称方式构建的原因）
	- **正交对角化定义**：$A \in \mathbb{R}^{n \times n}$，存在一个 正交矩阵 $P$ 和 对角矩阵 $D$，使得：$P^TAP = Λ$ 或者 $A = PΛP^T$    --- **注意：矩阵的正交对角化的算法**
	- **举例**：给定矩阵 $A$ ，分析沿着特征向量方向变换后的结果：$A = \begin{bmatrix} 5 & 2 \\2 & 2 \end{bmatrix}$
		1. **求特征值**：$\lambda_1 = 6, \lambda_2 = 1$   -- 通过特征方程 $(A - \lambda I_n)x = 0$ 求解特征值
		2. **求特征向量**：$\vec{u}_1 = \frac{1}{\sqrt{5}}\begin{pmatrix} 2 \ 1 \end{pmatrix}$，$\vec{u}_2 = \frac{1}{\sqrt{5}}\begin{pmatrix} 1 \ -2 \end{pmatrix}$   -- 特征方程代入特征值求解特征向量
		3. **构造正交矩阵**：$P = \frac{1}{\sqrt{5}}\begin{bmatrix} 2 & 1 \\1 & -2 \end{bmatrix}$，可以通过 $P^T P = I$ 来验证是否正交
		4. **二次型变换**：
			- **原始二次型**：$f(x_1,x_2) = 5x_1^2 + 4x_1x_2 + 2x_2^2$，有交叉项，**图形比较复杂**
			- **坐标变换**：$y = P^Tx \Rightarrow$ $\begin{pmatrix} y_1 \\ y_2 \end{pmatrix} = \frac{1}{\sqrt{5}}\begin{bmatrix} 2 & 1 \\1 & -2 \end{bmatrix}\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$，即：$y_1 = \frac{2x_1 + x_2}{\sqrt{5}}$，$y_2 = \frac{x_1 - 2x_2}{\sqrt{5}}$
			- **变换后二次型**：
				1. 根据 $x^T Ax = y^T Λy$，$f(y_1,y_2) = 6y_1^2 + y_2^2$
				2. 根据 $x^T Ax = \Sigma_{i=1}^n \lambda_i \cdot y_i^2$，直接写出：$f(y_1,y_2) = 6y_1^2 + y_2^2$
			- 可见，变换后的二次型 **没有了交叉项**，称为了 **标准的椭圆**
			- **验证**：
				1. 取 $\vec{x} = \begin{pmatrix} 1 \ 2 \end{pmatrix}$
				2. **原始二次型**：$f(x,y) = 5x_1^2 + 4x_1x_2 + 2x_2^2 = 21$
				3. 在特征向量方向变换后的坐标：$y_1 = \frac{4}{\sqrt{5}}, y_2 = \frac{-3}{\sqrt{5}}$
				4. **新二次型**：$f(y_1,y_2) = 6y_1^2 + y_2^2 = 21$
				5. **结论：正交变换的核心性质是保持函数值不变，但是改变了观察的视角，简化了形式，保持了本质**
- **常见二次型**：

	- $z = 3x_1^2 + 7x_2^2$          -- **开口朝上的 椭圆抛物面**
	- $z=-3x_1^2 - 7x_2^2$       -- **开口朝下的 椭圆抛物面**

| ![[a4280cc2-023b-47a3-85e2-6b8bfbe3b9e2.png]] | ![[2ab0f5db-195c-4cf7-b141-1c19f3654ca5 1.png]] |
| --------------------------------------------- | ----------------------------------------------- |

- $z = 3x_1^2$                       -- **开口朝上的圆柱抛物面**

![[f13114b1-aeb9-4c4c-bf2c-a8b7c97f60ce.png]]

- $z = 3x_1^2 -7x_2^2$             -- **双曲面**

![[3541519c-2171-4393-997c-72263eb85924.png]]

### 5. 奇异值分解



### 6. 常用性质

- 向量一般指的是 **列向量**
- 对称矩阵：$A^T = A$
- 向量正交：$v1 \perp v2 \Rightarrow v_1^Tv_2 = 0$
- 特征子空间正交：特征子空间 $E_{\lambda_1},E_{\lambda_2}$ 正交指的是 任意 $v_1 \in E_{\lambda_1}, v_2 \in E_{\lambda_2}$，都有 $v_1^Tv_2 = 0$
- **正交矩阵的逆**：$U^{-1} = U^{T}$
