
### 1. 线性变换

- **设** ：$A \in \mathbb{R}^{m \times n},x \in \mathbb{R}^n$
	- $A$ 可以看成是 $n$ - 维 列向量 $A = [a_1,...,a_n],a_i \in \mathbb{R}^m$
	- $x = (a_1,...,a_n)^T$
- **线性变换**：$Ax = b = [b_1,...,b_n] = x_1a_1 + ... + x_na_n$
	- **列空间**：$Col(A)$ - 是由 $A$ 的列向量 $a_1,...,a_n$ 张成的空间
		- 假设 $m > n$
		- 如果 $dim(Col(A)) = n$，那么列向量张成了 $\mathbb{R}^m$ 空间的一个 **$n$ - 维子空间**
		- 如果 $dim(Col(A)) = m$，那么列向量张成了整个 $\mathbb{R}^m$ 空间
	- **线性组合**：$Ax = x_1a_1 + ... + x_na_n$ 是 $A$ 的列向量的线性组合，$Ax \in Col(A)$
	- **空间变换**：$A \in \mathbb{R}^{m \times n}$ 定义了由 $\mathbb{R}^n$ 空间到 $\mathbb{R}^m$ 空间的一个变换（线性映射）
		- $x \in \mathbb{R}^n \mapsto Ax \in \mathbb{R}^m$
	- **线性独立**：如果 $a_1,...,a_n$ 线性无关（**线性独立**），它们构成了 $Col(A)$ 中的**一组基**
		- $rank(A) = n$
	- **基变换和向量变换**：
		- **主动视角**：参考系没有变化，向量 $x$ 在算子 $A$ 的作用下拉伸变换得到了新的向量 $b$
		- **被动视角**：向量没有变化，在新基 $A$ 下，向量坐标发生了变化 $A^{-1}x$ 是新基下的坐标
			- 如果 $A$ 是特征向量构成的矩阵，$A^{-1}x$ 得到了向量 $x$ 在特征向量基下的坐标
- **行列式**：
	- $det(A) \neq 0, A \in \mathbb{R}^{n \times n}$ - $A$ 的**列向量线性无关**，$A$ **可逆**
	- $|det(A)|$ - 体积的**缩放倍数**，$det(A)$ - 符号决定是否翻转

### 2. 特征值和特征向量

- **欧几里得范数**：
	- **非负性** ： $||v|| \geq 0$
	- **正定性**：$||v|| = 0$ 仅当 $v = 0$
	- **齐次性**：$||cv|| = |c| \cdot ||v||$
- **特征向量**：
	- 设 $A \in R^{n \times n}$，存在一个非零向量 $x \ne 0 \in R^n$  称为 $A$  的特征向量，使得  $Ax = \lambda x$
- **特征空间**：矩阵 $A \in R^{n \times n}$ 的特征空间 $E_{\lambda}$ 定义为所有特征向量（以及零向量） 的集合
	- $E_{\lambda} = \{x : Ax = \lambda x\}$
	- 一个特征值可以对应多个特征向量，一个特征向量对应一个特征值
	- $E_{\lambda}$ 是 $R^{n}$ 的一个子空间，满足：包含零向量，加法封闭性，数乘封闭性
- **特征方程**：$(A - \lambda I_n)x = 0$
	- 方程有**平凡解** ===> $det(A - \lambda I_n) = 0$
- **特征多项式**：$p(\lambda)=c_n​\lambda^n+c_{n−1}​\lambda^{n−1}+⋯+c_1​\lambda+c_0$
- **矩阵对角化条件**：$A \in \mathbb{R}^{n \times n}$
	- 有 $n$ 个不同的特征值 $\lambda_1,...,\lambda_n$
	- **对称矩阵**
	- 存在 n 个**线性无关** 的 **特征向量**
	- 存在一个**可逆矩阵** **P** 和一个 **对角矩阵** Λ，使得 $P^{-1}AP = Λ$
	- $\lambda_1,...\lambda_r$ 是其不同的特征值，A 可对角化当且仅当存在一个由特征向量构成的基，等价于：$dim(E_{\lambda_1}) + ... + dim(E_{\lambda_r}) = n$
- $det(A) = det(P Λ P^{-1}) = det(P)det(Λ)\frac{1}{det(P)}=det(Λ)$
- 设 $A = PΛP^{-1}, k \in \mathbb{N}, A^k = PΛ^kP^{-1}$
- **几何重数**：如果一个特征空间 $E_{\lambda}$ 的维数是 $k$，那么对应的特征值在矩阵 Λ 中出现 k 次。我们也称数字 k 为
- **求 $Ax$ 在特征基下的坐标：**
	- B 是 A 的特征向量构成的矩阵 - 特征基
	- $x$ 在特征基下的坐标： $[x]_B = B^{-1}x$
	- $A$ 在特征基下的矩阵：$[A]_B = B^{-1}AB = P$ - 如果矩阵可以对角化，P是对角矩阵
	- $Ax$ 在特征基下的坐标：$[Ax]_B = [A]_B [x]_B = B^{-1}AB[x]_B$

### 3. 正交和投影

- **正交矩阵的逆**：$U^{-1} = U^{T}$
- **内积**：$\langle v,w \rangle = v^Tw$
	- **基本性质**：
		- **对称性**：$\langle v,w \rangle = \langle w,v\rangle$
		- **分配律**：$\langle u,v+w \rangle = \langle u,v \rangle + \langle u,w\rangle$
		- **齐次性**: $\langle cv,w \rangle = c\langle v,w \rangle$
		- **正定性**：$\langle v,v \rangle \geq 0$  且 $\langle v,v \rangle=0$，那么 $v= 0$
	- **正交和内积**：$\langle v,w \rangle = 0$
		- $\theta = cos^{-1}(\frac{<v,w>}{||v|| \cdot ||w||})$
		- v ⊥ w
		- 零向量和 **任何向量** 正交
	- **范数和内积**：$||v|| = \sqrt{\langle v,v \rangle}$
	- **勾股定理**：对于正交向量 $v,w$，$||v+w||^2 = ||v||^2 + ||w||^2$
	- **柯西-施瓦茨不等式**：$\langle v,w \rangle \leq ||v|| \cdot ||w||$
	- **三角不等式**：$||u+v|| \leq ||u|| + ||v||$  --- 可以通过基本性质和柯西-施瓦茨不等式来证明
- **正交矩阵**：正交矩阵一定是方阵
	- **正交/可逆/转置**： U ∈ ℝⁿˣⁿ 是正交的，当且仅当 U 可逆且 U⁻¹ = $U^T$，且 $U^{-1}$ 和 $U^T$ 正交
	- **正交矩阵的行列式** $det(A) = ±1$
	- **正交变换后向量长度不变**：$‖Ux‖ = ‖x‖$ 对所有 $x \in \mathbb{R}^n$   -- 正交变换是旋转变换
	- **正交矩阵范数**：$||U|| = U^TU$
- **正交空间**：
	- **内积空间**：配备了内积的向量空间 $(V, \langle \cdot, \cdot \rangle)$ 称为**内积空间**
	- **正交补**：设 W 是 V 的子空间，那么我们称集合  $W^⊥ = {v ∈ V : v ⊥ W}$  
	- **补空间**：设 W 是 V 的子空间，那么 $W + W^⊥ = V$  $, W ∩ W^⊥  = \emptyset$
	- **空间正交**：W ⊥ U 可以写为 W ⊕ U
- **正交基**：在正交基下，要找向量 $v$ 在某个基向量 $v_j$ 方向上的坐标，只需要：
	- $v = c_1v_1 + ... + c_nv_n$
	- 其中，$c_j = \frac{<v,v_j>}{<v_j,v_j>}$
	- 如果是正交规范基，$c_j = <v,v_j>$，因为在正交规范基下 $<v_j,v_j> = 1$
	- 进而，$v = v_1e_1+...+v_ne_n$
- **投影**：
	- 任何向量都可以唯一分解为：**投影部分**（$v̂ \in W$）和 **垂直部分**（$z \in W^⊥$）
	- 对于每个 $v \in V$ 和子空间 $W$，我们能找到 $v̂ \in W$ 和 $z \in W^⊥$，使得：$v = v̂ + z$
		- $v$ - **任意向量**，$v̂$ - $v$ 在 $W$ 上的**投影**， $W$ - **投影空间**，$W^⊥$ - **正交补**，$v - v̂$ - 残差
	- $v$ 在 $W$ 上投影的向量 $v̂$ 是投影方向 $u \in W$ 的 $c$ 倍，$c = \frac{<u,y>}{<u,u>}$ 或者  ，**投影是u的标量倍**
	- **投影定理**：
		1. 每个 $v \in V$ 都可以唯一地表示为  $v=\hat{v}+z$，其中 $\hat{v} \in W$ 且 $z \in W^{\perp}$
		2. 对于 $W$ 的每个正交基 $\{w_1,...,w_k\}$ 都有：$\hat{v} = proje_W = \frac{\langle v,w_1 \rangle}{\langle w_1,w_1 \rangle}w_1 + ... + \frac{\langle v,w_k \rangle}{\langle w_k,w_k \rangle}w_1$
![[7d716ea9-f220-45e8-b9d2-0f38cb7fdc29.png]]
- **Gram-Schmidt正交化**：每个向量空间都有一个正交基，我们从任意基 $\{v_1,...,v_n\}$ 开始，将其转换为由 **正交向量** 组成的基 $\{u_1,...,u_n\}$
	- **标准正交基** $\{w_1,...,w_n\}$，其中将 **正交基** 的向量标准化：$w_j = \frac{u_j}{||u_j||}$
![[95dd46de-ca84-4a78-8b59-bce981541391.png]]
- **最小二乘法**：
	- 给定 n 组数据 $(y_1,x_1)...(y_n,x_n) \in \mathbb{R}^{p+1}$，我们希望通过 $x \in \mathbb{R}^p$  来 **预测**  $y$，等价于寻找 $\beta \in \mathbb{R}^p$ 满足：$y_i = x_i^T\beta + \epsilon_i$,，其中，$\epsilon_i$ 是代表误差。不同的 $\beta$ 会带来不同的误差，需要找到可以让误差 $\epsilon$ 最小的 $\beta$。
	- **问题的形式化定义**：给定一个线性方程组 $Ax = b$，其中 $A \in \mathbb{R}^{m×n}$ 且 $b \in \mathbb{R}^m$，我们想要找到一个向量 $\hat{x} \in \mathbb{R}^n$，使得 $||A\hat{x} - b||$ 最小（等价于  ||Ax - b||² = ε₁² + ⋯ + εₙ² 最小）
		- $A$ - 代表 $x$ , 即给定的数据，例如观测数据
		- $\hat{x}$ - 代表 $\beta$ ，理解为一个预测向量（参数）
		- $b$ - 代表观测到的“**结果**”
		- $||Ax-b||$ - 代表在 $\beta$ 作用下，**理论结果** 和 **实际结果** 之间的 **误差**
	- **问题解决**：
		1. 首先，向量 $b$ 在 $col(A)$ 上进行投影得到：$\hat{b}$
		2. 其次，线性方程组求解：$A\hat{x} = \hat{b}$，得到 $\hat{x}$ 是最佳的参数
		3. **定理**：求 $x = (A^TA)^{-1}A^Tb$ 得到**最佳近似的参数**
- **正交对角化**：
	- **定理**：$A \in \mathbb{R}^{n \times n}$，存在一个 **正交矩阵** $P$ 和 **对角矩阵** $D$，使得
	
		- $P^TAP = D$
		- $A = PDP^T$

	- 正交对角化条件：**当且仅当 A 是对称的**

		**证明**：$A^T = (PDP^T)^T = (P^T)^T \cdot D^T \cdot P^T = PDP^T = A$

	- 对称矩阵的所有**特征值都是实数**
- **谱分解**：
	- 通过谱分解，矩阵被分解为在 **不同特征方向上** 的 **投影操作** ($p_i p_i^T$)
	- 矩阵作用是各个特征方向上变换的叠加
	- 每个特征方向上的投影被对应的特征值缩放
	- **分解算法**：
		1. 计算 **特征值** $\lambda_i$ 和 **正交特征向量矩阵 P**
		2. **谱分解公式**：$A = P\Lambda P^T$ 可以重写为 $A = \sum_{i=1}^n \lambda_i p_i p_i^T$
![[60a9c81f-44c1-4456-807b-85571e7c2658.png]]


### 4. 二次型

- **我们的研究对象**：形如 $ax^2 + bxy + cy^2 + dx + ey + c = 0$ 的函数
	- 只有二次项，例如 $ax^2$ 或者 $bxy$ 会影响函数图像
	- 只含有 **二次项的函数** 我们称之为 **二次函数**
- **二次型的矩阵写法**：$\vec{x}^T \cdot A \cdot \vec{x}$
	- 我们通过微积分方式研究曲线（面）可以通过研究矩阵的方式进行，这样在大量参数的场景下（例如 **几千亿** 个神经网络参数）会更方便
	- **例如**：$f(x,y) = ax^2 + 2bxy + cy^2$
	- **矩阵改写**：
	$$\vec{x} = \begin{bmatrix} x \\ y \end{bmatrix}, \quad A = \begin{bmatrix} a & b \\b & c \end{bmatrix}$$
	$$f(x,y) = \begin{bmatrix} x & y \end{bmatrix} \cdot \begin{bmatrix} a & b \\b & c \end{bmatrix} \cdot \begin{bmatrix} x \\ y \end{bmatrix}$$
	- **注意**：为了方便处理，我们把交叉项的系数拆为 $2b$ 的形式，例如 $3xy$ 我们拆为 $2(\frac{3}{2})xy$
	- **对称矩阵**：不难看出，以上的矩阵改写方式，矩阵 $A$ 称为了 **对称矩阵** 的形式，对称矩阵在今后的应用中有很多好处，包括：其特征值是实数，图形上只有拉伸而没有旋转（虚特征值会导致图形旋转）
	- $n$ **维推广**：
	$$\vec{x} = \begin{bmatrix} x_1 & x_2 & ... & x_n \\ \end{bmatrix}, \quad A = \begin{bmatrix} A_{11} & A_{12} & ... & A_{1n} \\ ... & A_{22} & ... & ... \\ ... & ... & ... & ... \\ ... & ... & ... & ... \\ A_{n1} & ... & ... & A_{nn}\end{bmatrix}$$
$$f(x_1,x_2,...,x_n) = A_{11}x_1^2 + A_{22}x_2^2 + ... + A_{nn}x_n^2 + (A_{12}A_{21})x_1x_2 + (A_{34}A_{43})x_3x_4 + ...$$

此处，如果我们采用 **对称矩阵** 形式，交叉项 $A_{12} = A_{21}$，以上通项可以写为 $2A_{12}x_1x_2$

- **二次型定义及性质**：

	- **因为**：$Q(x,y) = \vec{x}^T \cdot A \cdot \vec{x}$
		- $A\vec{x}$ - 向量线性变换，**结果是一个向量**
		- $\vec{x}^T \cdot A\vec{x}$ - 向量的转置和向量相乘，是一个 “**内积**”，结果是一个**实数**
	- **所以**：$R^n$ 上的 **二次型** 是 **一个函数** ，由 $Q(x) = x^T Ax$ 给出，其中 $A \in R^{n \times n}$ 
	- **特性**：二次型 $Q(x) > 0$，说明 $\vec{x}^T$ 和 $A\vec{x}$ 两个向量夹角是锐角，即矩阵 $A$ 作用于向量 $\vec{x}$ 后（线性变换）和原始向量 “**同向**”。同理，$Q(x) < 0$，说明夹角是钝角，也就是 “**反向**”，$Q(x) = 0$，说明是直角，也就是正交变换。
		- **内积的几何公式**：$\vec{a} \cdot \vec{b} = ||\vec{a}|| \cdot ||\vec{b}|| \cdot cos\theta$
		- $cos$ 函数在 $(0, \frac{\pi}{2})$ 大于 0, $(\frac{\pi}{2},\pi)$ 小于 0
	- **正定性**：
		- $\ge 0$ ：正定
		- $> 0$ ：半正定
		- $< 0$ ：负定
		- $\leq 0$ ：半负定
		- 不确定符号：不定
	- **最大值与最小值**：
		- 正定：**开口朝上，有全局极小值**
		- 负定：**开口朝下，有全局极大值**
		- 半正定：**开口朝上，有极小值，但不是严格极小值**
		- 半负定：**开口朝下，有极大值，但不是严格极大值**
		- 不定：**既没有极小值也没有极大值**

- **正定性的判定准则**：**特征值** $\lambda_1,\lambda_2,...$（矩阵的对角线）是否全为正数
- **主轴定理**：$A \in ℝ^{n \times n}$ 是 一个 **实对称矩阵**，通过 P **正交地对角化** 为矩阵 $Λ = diag(λ₁,...,λₙ)$，那么对于 $y = P^T x$，有：$x^T Ax = y^T Λy = \Sigma_{i=1}^n \lambda_i \cdot y_i^2$
	- **谱定理**（Spectral Theorem）：**实对称矩阵** **必定** 可以正交对角化（这也就是上面构建二次型矩阵用对称方式构建的原因）
	- **正交对角化定义**：$A \in \mathbb{R}^{n \times n}$，存在一个 正交矩阵 $P$ 和 对角矩阵 $D$，使得：$P^TAP = Λ$ 或者 $A = PΛP^T$    --- **注意：矩阵的正交对角化的算法**
	- **举例**：给定矩阵 $A$ ，分析沿着特征向量方向变换后的结果：$A = \begin{bmatrix} 5 & 2 \\2 & 2 \end{bmatrix}$
		1. **求特征值**：$\lambda_1 = 6, \lambda_2 = 1$   -- 通过特征方程 $(A - \lambda I_n)x = 0$ 求解特征值
		2. **求特征向量**：$\vec{u}_1 = \frac{1}{\sqrt{5}}\begin{pmatrix} 2 \ 1 \end{pmatrix}$，$\vec{u}_2 = \frac{1}{\sqrt{5}}\begin{pmatrix} 1 \ -2 \end{pmatrix}$   -- 特征方程代入特征值求解特征向量
		3. **构造正交矩阵**：$P = \frac{1}{\sqrt{5}}\begin{bmatrix} 2 & 1 \\1 & -2 \end{bmatrix}$，可以通过 $P^T P = I$ 来验证是否正交
		4. **二次型变换**：
			- **原始二次型**：$f(x_1,x_2) = 5x_1^2 + 4x_1x_2 + 2x_2^2$，有交叉项，**图形比较复杂**
			- **坐标变换**：$y = P^Tx \Rightarrow$ $\begin{pmatrix} y_1 \\ y_2 \end{pmatrix} = \frac{1}{\sqrt{5}}\begin{bmatrix} 2 & 1 \\1 & -2 \end{bmatrix}\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$，即：$y_1 = \frac{2x_1 + x_2}{\sqrt{5}}$，$y_2 = \frac{x_1 - 2x_2}{\sqrt{5}}$
			- **变换后二次型**：
				1. 根据 $x^T Ax = y^T Λy$，$f(y_1,y_2) = 6y_1^2 + y_2^2$
				2. 根据 $x^T Ax = \Sigma_{i=1}^n \lambda_i \cdot y_i^2$，直接写出：$f(y_1,y_2) = 6y_1^2 + y_2^2$
			- 可见，变换后的二次型 **没有了交叉项**，称为了 **标准的椭圆**
			- **验证**：
				1. 取 $\vec{x} = \begin{pmatrix} 1 \ 2 \end{pmatrix}$
				2. **原始二次型**：$f(x,y) = 5x_1^2 + 4x_1x_2 + 2x_2^2 = 21$
				3. 在特征向量方向变换后的坐标：$y_1 = \frac{4}{\sqrt{5}}, y_2 = \frac{-3}{\sqrt{5}}$
				4. **新二次型**：$f(y_1,y_2) = 6y_1^2 + y_2^2 = 21$
				5. **结论：正交变换的核心性质是保持函数值不变，但是改变了观察的视角，简化了形式，保持了本质**
- **常见二次型**：

	- $z = 3x_1^2 + 7x_2^2$          -- **开口朝上的 椭圆抛物面**
	- $z=-3x_1^2 - 7x_2^2$       -- **开口朝下的 椭圆抛物面**

| ![[a4280cc2-023b-47a3-85e2-6b8bfbe3b9e2.png]] | ![[2ab0f5db-195c-4cf7-b141-1c19f3654ca5 1.png]] |
| --------------------------------------------- | ----------------------------------------------- |

- $z = 3x_1^2$                       -- **开口朝上的圆柱抛物面**

![[f13114b1-aeb9-4c4c-bf2c-a8b7c97f60ce.png]]

- $z = 3x_1^2 -7x_2^2$             -- **双曲面**

![[3541519c-2171-4393-997c-72263eb85924.png]]

### 5. 奇异值分解

- **一些必须熟练的定理：**
	- **矩阵的秩序**：$rank(A)$，线性无关的行/列数，行秩 = 列秩 = 秩
	- **实对称矩阵**  一定可以正交对角化  $A = PΛP^T$
	- **矩阵变换**：半正定对角矩阵（所有特征值都是非负数）代表 **伸缩变换**   正交矩阵代表 旋转变换
	- **对称矩阵** 的 **特征向量 互相垂直**
	- **维度消除矩阵**：$\begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0\end{bmatrix}$
	
				$\begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0\end{bmatrix} \begin{bmatrix} 1 \\ 2 \\ 7\end{bmatrix} =  \begin{bmatrix} 1 \\ 2\end{bmatrix}$

	- **维度增加矩阵**：$\begin{bmatrix} 1 & 0  \\0 & 1 \\ 0 & 0 \end{bmatrix}$


				$\begin{bmatrix} 1 & 0  \\0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 3  \\ 5 \end{bmatrix} = \begin{bmatrix} 3  \\ 5 \\ 0 \end{bmatrix}$

	- **特征向量/值的拉伸特性**：
		- **只有方阵才有特征值/特征向量，因为非方阵，$A\vec{x} = \lambda \vec{x}$ 左右维度不一样**
		- **矩阵A**：看成是一种线性 **变换方式**，在不同的方向上行为不一样
		- **特征值/向量定义**：$A\vec{x} = \lambda \vec{x}$，矩阵 $A$ 作用于特征向量 $\vec{x}$，向量被 **伸缩** 了 $|\lambda|$ 倍，也就是 $|\lambda|$ 代表了 **伸缩** 倍
		- **绝对值最大的特征值**： $\lambda_{max}$ 相应的特征向量指示了 变换$A$ 的 **拉伸** 最强烈的方向
		- **绝对值最小的特征值**： $\lambda_{min}$ 代表了 变换$A$ 的 **压缩** 最强烈的方向
		- **那么，非方阵怎么 寻找这个方向？**
		
- **从例子开始**：

| ![[b4f707e5-bb5d-4830-ab84-874abca307a0.png]] |
| --------------------------------------------- |
|                                               |
	- **左侧球体**：$\vec{x} \in \mathbb{R}^3$，**球面上各个方向的向量长度相同**
	- **变换矩阵**：$A \in \mathbb{R}^{2 \times 3}$ - **将 3 维 变换为 2 维**
	- **变换**：$Ax \mapsto \mathbb{R}^2$ - 三维的**球体**变换为两维后成了平面的**椭圆**，椭圆边缘有 **长轴** 和 **短轴** 方向

- **奇异值**：

	- 为了找到拉伸方向，问题等价于寻找 $||Ax||^2$ 的极值点
	- $||Ax||^2 = (Ax)^T(Ax) = x^T(A^TA)x$，很明显，这是一个关于 $A^TA$ **的二次型**，且如果 $A \in \mathbb{R}^{m \times n},A^TA \in \mathbb{R}^{n \times n}$，即 $A^TA$ **变成了熟悉的方阵，且是对称的**，即 **实对称方阵**
	- $\lambda_1 \geq...\geq\lambda_n$ 是 $A^TA$ 的特征值，那么 $\sigma_i = \sqrt{\lambda_i}$ 是矩阵 $A$  的**奇异值**，而且：
			$\sigma_1 = \sqrt{\lambda_1} = \sqrt{max_{||x|| = 1}x^TA^TAx} = max_{||x||=1}||Ax||$
		- 即 **最大的奇异值** 是：
			1. 矩阵 $A$ 针对 范数为1 的向量的变换后的向量中范数最大的一个值
			2. $A^TA$ 这个实对称矩阵的二次型（$x^TA^TAx$）函数的 **最大值**

- **奇异向量**：与奇异值 $\sigma_i$ 对应有左和右奇异向量，其中
	- **右奇异向量**：$A^TA$ 的 **归一化** 的特征向量 $v_i$ 
	- **左奇异向量**：$u_i = \frac{Av_i}{\sigma_i}$
	- **正交性**：右奇异向量是正交的（$A^TA$ 是实对称矩阵），根据此，左奇异向量也正交

- **奇异值与正交基**：
	- $r = rank(A)$ - 非零奇异值的个数
	- $\{v_1,...,v_r\}$ 是 $col(A^T)$ 的正交基
	- $\{Av_1,...,Av_r\}$ 是 $col(A)$ 的正交基

- **奇异值分解目的**：对于任意矩阵 $M$，可以变换为 $M = U \cdot \Sigma \cdot V^T$
	- $U$ - 正交矩阵，代表旋转
	- $\Sigma$ - 正定对角矩阵，代表拉伸，对角元素表示拉升强度
	- $V^T$ - 正交矩阵，代表旋转

- **奇异值分解**：设 $A \in \mathbb{R}^(m×n)$ 是一个秩为 $rank(A) = r$ 的矩阵，那么 A 可以分解为：
	-  $A = U \cdot \Sigma \cdot V^T$
	- $V = (v_1 ...  v_n) \in \mathbb{R}^{n \times n}$ 是一个**正交矩阵**，其列包含右奇异向量
	- $U = (u_1... u_m) \in \mathbb{R}^{m \times m}$ 是一个**正交矩阵**，其列中前 $r$ 个左奇异向量 $u_1,...u_r$ 扩展为 $\mathbb{R}^m$ 的一组正交基 ${u_1, ..., u_r, u_{r+1}, ..., u_m}$
	- $\Sigma \in \mathbb{R}^{m \times n}$ 是对角矩阵

				$\begin{bmatrix} diag(\sigma_1,...,\sigma_r) & 0_{r \times (n-r)}  \\0_{(m-r) \times r} & 0_{(m-r) \times (n-r)} \end{bmatrix}$

- **简化SVD**：完整的 $\Sigma$ 是 m x n 矩阵，只有前 $r$ 个对角元素非零，其余全是零，造成浪费，可以简化为：$A = U_r \cdot \Sigma_r^{-1} \cdot V_r^T$

- **综合举例**：一个 2 x 3 阶矩阵 $A = \begin{bmatrix} 1 & 0 & 1 \\0 & 1 & 1 \end{bmatrix}$，对齐进行SVD分解
	1. 计算 $B = A^TA = \begin{bmatrix} 1 & 0 & 1 \\0 & 1 & 1 \\1 & 1 & 2 \end{bmatrix}$
	2. 求特征值和特征向量：$\begin{bmatrix} 1-\lambda & 0 & 1 \\0 & 1-\lambda & 1 \\1 & 1 & 2-\lambda \end{bmatrix} = \lambda(1-\lambda)(\lambda-3) = 0$，得到：

		**特征值**：$\lambda_1 = 3, \lambda_2 = 1, \lambda_3 = 0$
		**奇异值**：$\sigma_1 = \sqrt{3}, \sigma_2 = 1, \sigma_3 = 0$		
		
	3.  **求右奇异向量**：
		- $(A^TA - 3I)\vec{v_1} = 0$，得到 $\vec{v_1} = \begin{pmatrix} 1 \\ 1 \\ 2\end{pmatrix}$，归一化后， $\vec{v_1} = \begin{pmatrix} \frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{6}} \\ \frac{2}{\sqrt{6}}\end{pmatrix}$
		- $(A^TA - 1I)\vec{v_2} = 0$，得到 $\vec{v_2} = \begin{pmatrix} 1 \\ -1 \\ 0\end{pmatrix}$归一化后， $\vec{v_2} = \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}} \\ 0\end{pmatrix}$
		- $(A^TA - 0I)\vec{v_1} = 0$，得到 $\vec{v_1} = \begin{pmatrix} 1 \\ 1 \\ -1\end{pmatrix}$归一化后， $\vec{v_3} = \begin{pmatrix} \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}} \\ \frac{-1}{\sqrt{3}}\end{pmatrix}$
	4. 构造 $V$ 矩阵：$\begin{bmatrix} \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{6}} & \frac{-1}{\sqrt{2}} & \frac{1}{\sqrt{3}} \\ \frac{2}{\sqrt{6}} & 0 & \frac{-1}{\sqrt{3}}\end{bmatrix}$
	
	5. 计算左奇异向量：由于 $A$ 是 2 x 3阶矩阵，秩为 2，所以左奇异矩阵 r = 2
	
		- $u_1 = Av_1/\sigma_1 = \frac{\begin{bmatrix} 1 & 0 & 1 \\0 & 1 & 1 \end{bmatrix} \cdot \begin{pmatrix} \frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{6}} \\ \frac{2}{\sqrt{6}}\end{pmatrix}}{\sqrt{3}} = \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix}$
		- $u_2 = ...$


	6. 构造 $U$ 矩阵：$\begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{2}} \end{bmatrix}$

	7. 构造 $\Sigma$ 矩阵：$\Sigma = \begin{bmatrix} \sqrt{3} & 0 & 0 \\0 & 1 & 0 \end{bmatrix}$，带入了奇异值 $\sigma$
### 6. 迹和范数

- **迹的定义**：$A \in \mathbb{R}^{n \times n}$ （方阵）的 **”迹“** 定义为对角线元素之和，$tr(A) = \Sigma_{i=1}^n a_{ii}$
	- **转置性质**：$tr(A) = tr(A^T)$
	- **线性性质**：$\text{tr}(c_1 A + c_2 B) = c_1 \text{tr}(A) + c_2\text{tr}(B)$
	- **交换性质**：对于所有 $A \in \mathbb{R}^{m \times n}, B \in \mathbb{R}^{n \times m}$，有  $\text{tr}(AB) = \text{tr}(BA)$
- **矩阵范数**：
	- **从向量范数派生的矩阵范数**：$\begin{bmatrix} 1 & 2  \\3 & 5 \end{bmatrix}$ **转换为向量** $\begin{pmatrix} 1 & 3 & 2 & 4 \end{pmatrix}$
	- **正定性**：$||A|| = 0 \Leftrightarrow A = 0_{m \times n}$ 
	- **绝对齐次性**：$||cA|| = |c| ||A||$
	- **三角不等式**：$||A + B|| \leq ||A|| + ||B||$ 
- **常见范数**：
	- **Frobenius范数**：衡量矩阵的"**总能量**"，$||A||_F = \sqrt{\Sigma_i\Sigma_j a_{ij}^2} = \sqrt{tr(A^TA)}$
	- **谱范数**（Spektralnorm)：代表矩阵的最大"**拉伸因子**"，变换矩阵 $A$ 的最大范数是 $A^TA$ **方阵** 的 **最大特征值的平方根**，$||A||_2 := max_{||x||_2 = 1 ||Ax||_2} = \sigma_{max}(A) = \sqrt{\lambda_{max}(A^TA)}$      、
	- **列和范数**（Spaltensummennorm)：**每一列** 元素 **绝对值** 之和，取最大值。$||A||_1 := max_{||x||_1 = 1}||Ax||_1| = max_{1 \leq j \leq n} \Sigma_{i=1}^m |a_{ij}|$
	- **行和范数**（Zeilensummennorm)：**每一行** 元素 **绝对值** 之和，取最大值。$||A||_{\infty} := max_{||x||_{\infty} = 1}||Ax||_{\infty} = max_{1 \leq i \leq n}\Sigma_{j = 1}^n |a_{ij}|$ 
- **算子范数**：算子范数是 在所有非零向量x中，矩阵A **最大** 能将向量"**放大**"多少倍。
	- $||A||_V = max_{x \ne 0} \frac{||Ax||_V}{||x||_V} = max_{||x||_V = 1}||Ax||_V$
	- **瑞丽商**：$R_A(x) = \frac{||Ax||_V}{||x||_V}$，如果这个 **商很大**，则表示向量 **被强烈拉伸**
	- **举例**：
		- 矩阵 $A = \begin{bmatrix} 3 & 1  \\2 & 2 \end{bmatrix}$ 在 **2-范数** 下向量 $(1,0)^T$ 的瑞丽商
			1. 计算 $Ax = (3,2)^T$
			2. 计算 $||Ax||_2 = \sqrt{13}$
			3. 计算 $||x||_2 = 1$
			4. 所以，$R_1((1,0)^T) = \sqrt{13}$

### 7. 常用性质

- 向量一般指的是 **列向量**
- 对称矩阵：$A^T = A$
- 向量正交：$v1 \perp v2 \Rightarrow v_1^Tv_2 = 0$
- 特征子空间正交：特征子空间 $E_{\lambda_1},E_{\lambda_2}$ 正交指的是 任意 $v_1 \in E_{\lambda_1}, v_2 \in E_{\lambda_2}$，都有 $v_1^Tv_2 = 0$
- **正交矩阵的逆**：$U^{-1} = U^{T}$
