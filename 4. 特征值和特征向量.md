P74-94   9.11

### 0. 来龙去脉

1.  考虑在二维空间中的一个**线性变换**：
	- 将 x 坐标的基向量 $\hat{i}$ 变换到 $(3,0)$ 
	- 将 y 坐标的基向量 $\hat{j}$  变换到 $(1,2)$
2. 如果用矩阵表达这个变换就是：$\begin{pmatrix} 3 & 1 \\ 0 & 2 \end{pmatrix}$
3. 大部分向量在变换后 **离开了** 其张成的向量空间
4. 有些特殊的向量变换后停留在了其张成的空间中
5. 此时，矩阵对他们的作用只是拉伸或者压缩，相当于一个标量
6. 在这个例子中 $\hat{i}$ 就是两个这种特殊向量
7. 还有一个特殊向量 (-1,1) ，变换后还是停留在其张成空空间中
8. 这些特殊向量称为 **变换的特征向量**， 每个特征向量对应一个**特征值**， 描述变换后拉伸或者压缩的**比例**
9. $A\hat{v} = \lambda \hat{v}$   -- 左边是变换，右边是标量乘法，也就是拉伸压缩，$\hat{v}$ **非零向量** -- 几何意义！！！
10. 线性变换 **不一定** 都有特征向量 -- **没有实数解**
11. 存在**一个特征值**，但是**对应不同的特征向量**
12. **对角矩阵对角线的元素全都是特征值**


对于给定的（随机）矩阵 M，存在一个向量 q，使得 Mq = q，q 可以被视为平衡状态：**它不会被转移矩阵M改变**，我们将q称为M的特征向量。"特征"（Eigen）一词意在表达这是矩阵M的一个特性或特征



### 1. 特征向量

**特征向量定义**：设 $A \in R^{n \times n}$，存在一个向量 $x \ne 0 \in R^n$ 称为 $A$  的特征向量，使得  $Ax = \lambda x$

- A - 方阵
- x -  特征向量
- $\lambda$ - 特征值

理解：

- 用 $\lambda$ 来拉伸向量 x
	- $|\lambda| > 0$ - 拉伸了向量
	- $|\lambda| < 1$ - 压缩了向量
	- $|\lambda| = 0$ - 向量变成零向量
	- $\lambda < 0$ - 向量方向反转 
- 当矩阵A作用于它的**特征向量**时，结果就是简单地拉伸（或压缩）这个向量

![[d031e307-99b7-491c-ae1d-8ba2c0690404.png]]

![[66b8d1ad-7783-490b-b5db-5b28f170cf30.png]]

- 令 $x_1 = (1,0)^T, x_2=(0.5,0.5)^T$
- $A = (x_1,x_2), Av_1$ 可以看作是 A 的一个线性组合
- $Av1 = (1,0)^T \cdot -2 + (0.5,0.5)^T \cdot 2 = (-1, 1)^T = \frac{1}{2}v1$
- 可见，$v_2$ 是 A 的一个特征向量，特征值是 $\frac{1}{2}$
- 0 < **特征值** < 1，是一个**线性压缩**

- 同理，$v_2$ 也是A一个特征向量，特征值也是 $\frac{1}{2}$
- 计算 Au1，Au2 可知道 u1，u2不是特征向量

- 一个 n x n 矩阵，最多可以有 n 个线性无关的特征向量，可能对应 n 个不同特征值
- **倍数性**：如果 v 是特征向量，2v，3v，-v，kv 等都是特征向量，**特征值一样**
	- $A(cv) = c(Av) = c(\lambda v) = \lambda(cv)$

### 2. 特征空间

**特征空间定义**：矩阵 $A \in R^{n \times n}$ 的特征空间 $E_{\lambda}$ 定义为所有特征向量（以及零向量） 的集合。

	$E_{\lambda} = \{x : Ax = \lambda x\}$

**引理**：对于每一个矩阵 $A \in R^{n \times n}$ ，以及 $\lambda \in R$，$E_{\lambda}$ 是 $R^{n}$ 的一个子空间。

- 子空间：包含零向量，加法封闭性，数乘封闭性

**重要例子1**：

- 对角矩阵 $A = (x_1,x_2,x_3)$，其基向量 $e_1,e_2,e_3$，不难想象：
	- $A \cdot e_1 = x_1 \cdot e_1 = x_{11} \cdot e_1$
	- $A \cdot e_2 = x_2 \cdot e_2 = x_{22} \cdot e_2$
	- ...
- 所以，$e_1,e_2,e_3$ 是特征向量，特征值就是对角元素的值
- $A = ((2,0,0)^T,(0,-1,0)^T,(0,0,2)^T)$
- 有两个特征值：2，-1
- 每个特征值对应的特征向量构成的集合 ===> 特征空间
- $E_2 = span(e_1,e_3)$
- $E_{-1} = span(e_2)$

**重要例子2**：

- 其中同一特征值对应多个线性无关的特征向量，就是零矩阵
- $Ax = 0 = 0 \cdot x$
- $A = ((0,0)^T,(0,0)^T)$

**重要例子3**：

- $E_1 = span((1,1)^T), E_{-1} = span((1,-1)^T)$
- 空间 $E_1$ 包含沿镜像轴的所有向量，**这些向量在与 A 相乘时保持不变**
- 空间 $E_{-1}$ 包含垂直于镜像轴的所有向量，**这些向量在与 A 相乘时方向反转**

![[76776187-86ae-447d-a7bf-1d4f1f341344.png]]

![[59ce88c2-2bec-4de1-9f4d-d31d0a4e01f6.png]]  
这个特殊矩阵是让二维点（x,y) 变换 x 和 y 坐标

![[9fb038ad-e07c-47e7-85cf-e9bde26481b9.png]]

**！！！！大多数向量经过变换后会改变方向，但特征向量很特殊**

- **特征向量在变换后仍保持同一条直线上**（可能反向或缩放）
- 数学表达：Av = λv（只是缩放，不改变方向）

![[e047269f-c7a1-4789-b390-59a159d11074.png]]
### 3. 特征方程

**目的**：我们想要更一般地 **探讨特征值的存在性** 以及 **如何寻找特征值**

- **特征方程**：$Ax = \lambda x$
- **改写**：$(A - \lambda I_n)x = 0$ -- 注意需要乘以单元向量 $I_n$
- 如果 $\lambda$ 是特征值 ====> $(A - \lambda I_n)x = 0$ 方程有**平凡解** ===> $det(A - \lambda I_n) = 0$
- 一般化方法：对于矩阵，我们通过从所有对角元素中减去 $\lambda$ 来得到 $A - \lambda I_n$
- 然后计算行列式，行列式的 **零点** 就是 A 的特征值，由此可以看出，有 n 个特征值（n元矩阵对应n元方程组，有n个解）
- **特征多项式**：$p(\lambda)=c_n​\lambda^n+c_{n−1}​\lambda^{n−1}+⋯+c_1​\lambda+c_0$

![[ee3dd9aa-61c8-4eb1-8b67-0c2e1d6cb974.png]]


- **Tips1**：**三角矩阵的行列式** = **对角线元素的乘积**
![[65be0fe7-c2a7-422c-9f3f-8e974483b599.png]]

### 4. 等价性

- $\lambda = 0$  **的特殊意义**：
	- 整个空间 $E_0$ 被 ”压缩“ 到 原点
	- **体积坍缩** ： 行列式为0

**以下陈述等价**：

**(a) A 是可逆的。**
**(b) A^T 是可逆的。**
**(c) A 的化简行阶梯形是 I_n。**
**(d) 线性方程组 Ax = 0 只有平凡解。**
**(e) 线性方程组 Ax = b 对每个 b ∈ ℝ^n 都恰有一个解。**
**(f) 线性方程组 Ax = b 对每个 b ∈ ℝ^n 都有解。**
**(g) A 的列向量线性无关。**
**(h) A 的行向量线性无关。**
**(i) A 的列向量张成 ℝ^n。**
**(j) A 的行向量张成 ℝ^n。**
**(k) det(A) ≠ 0。**
**(l) rank(A) = n。**
**(m) ker(A) = {0}。**
**(n) λ = 0 不是 A 的特征值。**

**证明**：

- $\lambda = 0$  是特征值
- $det(A - 0 \cdot I) = 0$
- $det(A) = 0$
- A 不可逆

特征值/特征向量 = 找到数据的"**主要方向**"

大特征值 → 重要信息 → 必须保留 
小特征值 → 次要信息 → 可以丢弃（压缩） 
零特征值 → 无信息 → 一定丢失


- 矩阵行列式为0 ===> 有特征值0 ===> 有可能是投影矩阵（特征值只能为0和1） ===> 代入特征值为1计算行列式 

![[bba2369e-6625-446c-afe9-f71f2ab61879.png]]

投影矩阵 ==> 


### 5. 对角化

**如果我们选择特征向量作为基，线性变换会变得特别简单**

- 设 $v_1,v_2,...$ 是 A 的特征向量， $\lambda_1,\lambda_2...$ 是特征向量对应的特征值
- 所以，$\mathcal{B} = \{v_1,v_2,...,v_n\}$ 是**一组基**（特征基）
- 对于任意的一个**向量** $x \in R^n$ --- **x 是被变换的对象**
- 假设 B 是一个**变换矩阵**，由特征向量构成 $B = (v_1,...,v_n)$
- 得到：$x=B[x]_B$  ​，左右同时乘以 $B^{-1}$ 得到 ：$B^{−1}x=[x]_B$
	- $[x]_B$ 表示向量 $x$ 在基 $\mathcal{B}$ 下的坐标表示​
	- 任意一个向量 V = 特征矩阵 x 向量V的基

- 原本 向量 $x$ 要通过 矩阵 $A$ 来变换，这个变换会很复杂
- 现在，找到矩阵 A 的特征矩阵 B，找到矩阵 A 的特征值构建的基 $\mathcal{B}$（一个对角矩阵）
- 计算过程：
	- 首先计算向量 $x$ 在 基 $\mathcal{B}$ 下的坐标 $[x]_{\mathcal{B}}$
	- 向量 $x$ **等价于** 用 特征矩阵 B 来乘以 $[x]_{\mathcal{B}}$  ：$x = B[x]_{\mathcal{B}}$      !!!!!!!!!!!
	- 计算得到 $[x]_{\mathcal{B}} = B^{-1}x$
	- 因此，$[Ax]_{\mathcal{B}} = B^{-1}Ax = B^{-1}AB[x]_{\mathcal{B}}$
	- **可见，$[x]_{\mathcal{B}}$  在矩阵 $B^{-1}AB$ 的变换下变成 $[Ax]_{\mathcal{B}}$  -  也就是 Ax 变换可以通过乘以 $B^{-1}AB$ 得到 在特征基 下的坐标 ！！！！！！！！！！！！**

![[68a6366f-9b15-489a-bf3b-f058f637a3d4.png]]
从上面图可以看出 向量 $x$ 在特征矩阵 $B$ 和 $B^{-1}$ 作用下可以来回变换，因此 Ax 也可以在 $B$ 和 $B^{-1}$ 作用下来回变换！！！即将变换转换到特征空间下的方法

![[93fcf7e2-3bd8-4dba-9f21-2fe44a4a2f94.png]]

根据以上计算，可以得知 $B^{-1}AB$  的结果就是三角矩阵：

![[d15171cc-5c06-4523-9121-91522a6b0c85.png]]


以上过程，我们称为：“B **对角化**了矩阵 ”，反过来，我们也可以将 A = $BΛB^{-1}$，这也被称为**谱分解**

- **对角化**：$Λ = B^{-1} \cdot A \cdot B$，**在特征基下**：复杂的线性变换 → 简单的对角缩放
- **谱分解**：$A = B \cdot Λ \cdot B^{-1}$

![[29313d72-4b22-44fb-bf9e-c63dba2d5297.png]]

### 6. 可对角化性

**定义**：我们称矩阵 A 是**可对角化的**，如果存在一个**可逆矩阵** **P** 和一个 **对角矩阵** **D**，使得:

							$P^{-1}AP = D$

**定理**：矩阵$A \in \mathbb{R}^{n \times n}$ 可对角化的充要条件：**特征向量线性无关**

- 为了对角化一个矩阵，我们只需要找到 n 个线性无关的特征向量

**定理**：设 $\lambda_1 \neq ... \neq \lambda_n$ 是矩阵 A 的不同的特征值，那么对应的特征向量 $v_1,...,v_n$ 线性无关

**推论**：矩阵$A \in \mathbb{R}^{n \times n}$ 有 n 个不同的特征值，那么他可以被对角化

**定理**：设 矩阵$A \in \mathbb{R}^{n \times n}$ ，$\lambda_1,...\lambda_r$ 是其不同的特征值，A 可对角化当且仅当存在一个由特征向量构成的基，等价于：$dim(E_{\lambda_1}) + ... + dim(E_{\lambda_r}) = n$

**几何重数**：如果一个特征空间 $E_{\lambda}$ 的维数是 $k$，那么对应的特征值在矩阵 Λ 中出现 k 次。我们也称数字 k 为

### 7. 结论

对角化对于理解矩阵变换的作用特别有用：

- 在特征空间内，向量仅沿着特征向量方向被拉伸
- 变换到由特征向量构成的基的过程，我们也称为**主轴变换**
- 在统计学中，它作为**主成分分析（PCA）**发挥着重要作用
- 稍后会详细介绍。反过来，我们也可以将 A 写成 PΛP⁻¹，这也被称为**谱分解**。

$det(A) = det(P Λ P^{-1}) = det(P)det(Λ)\frac{1}{det(P)}=det(Λ)$

**定理**：$det(A) = \lambda_1 \cdot ... \cdot \lambda_n$ -- 因此，**行列式不过是特征值的乘积**

**定理**：设 $A = PΛP^{-1}, k \in \mathbb{N}, A^k = PΛ^kP^{-1}$

**定理**：设 $A = PΛP^{-1}, k \in \mathbb{N}, A^{-k} = PΛ^{-k}P^{-1}$


### 8. 复特征值


![[b59630e9-7672-4eae-bee1-7789606d5f0a.png]]

在这个例子中，我们看到两个特征值**互为共轭**。同样，对应特征向量中的分量也互为共轭。这是普遍成立的，直接由复数的运算规则得出

**定理**：设 A ∈ ℝⁿˣⁿ，λ 是 A 的一个特征值，v 是对应的特征向量。那么 $\bar{\lambda}$ 也是 A 的特征值，$\bar{v}$ 是对应的特征向量。

**只要记住，复特征值和特征向量可能会出现**!!!!


### 9. 应用

#### 9.1 PCA 主成分分析

主成分分析（英文：principal component analysis 或 PCA）是多元统计中最重要的工具之一。它有助于更容易地理解大型数据集中的关联，或在不丢失重要信息的情况下降低维度。

**基本思想**

假设我们的数据（相当于矩阵 A ）由 d 个变量组成。我们可以将一个实现写成向量 x ∈ ℝᵈ。对应的随机向量记为 X ∈ ℝᵈ。设 Σ = Cov(X) 为 X 的**协方差矩阵**。**对角线元素描述了各个变量的方差大小**。其他元素描述了变量之间是否存在依赖关系以及依赖的强度。

当 d 很大时，Σ 中包含了难以把握的大量信息。为了更容易理解数据中的变异性和关联性，我们可以对角化 Σ。为此我们找到特征向量 v₁, ..., vₙ 和对应的特征值 λ₁, ..., λₙ。然后有：$\Sigma = PΛP^{-1}$ 

其中 P = (v₁ ··· vₙ) 且 Λ = diag(λ₁, ..., λₙ)。矩阵 P 和 P⁻¹ 将我们带入特征向量基的坐标系。在那里，矩阵 Σ 有一个简单的表示 Λ。

不失一般性，**假设特征值按大小排序：λ₁ ≥ λ₂ ≥ ··· ≥ 0。那么可以证明，特征向量 v₁ 给出了变异性最大的方向**。同样，vₙ 是变化最少的方向。

因此，为了理解数据，我们可以只关注前几个特征向量的方向。

![[f40806bc-082a-4daa-b146-6a7555aeb450.png]]


- 左图：x1 和 x2 是相关的（斜着）
- 右图：在PCA主成分方向，数据不相关（横着）

**载荷矩阵**：

- **正载荷（如0.70）**：该变量与主成分正相关
- **负载荷（如-0.30）**：该变量与主成分负相关
- **载荷绝对值大（>0.5）**：该变量对主成分贡献大
- **载荷绝对值小（<0.3）**：该变量对主成分贡献小


#### 9.2 马尔可夫链和谷歌

在第2.3.3节中，我们已经介绍了动态系统和马尔可夫链。现在我们想通过一个每天都会遇到的例子来深入探讨这个主题。谷歌的搜索算法本质上基于马尔可夫链。谷歌创始人之一拉里·佩奇在千禧年之交前不久发明了PageRank算法。为了理解这个想法，我们首先限定在一个简化的场景中。

**随机模型**

假设针对某个搜索词只有5个网页。搜索引擎现在试图根据网页的重要性将它们排序成一个排名列表。其中一些网页有指向其他网页的链接。大致的想法是，被更多链接指向的页面更重要。为此，我们想象一个愚蠢的用户Bob。Bob无休止地浏览网页（由5个页面组成），并随机点击链接。Bob在t步之后处于5个网页之一的概率可以表示为一个状态向量。我们通过乘以一个转移概率矩阵来建模时刻t+1的概率。


