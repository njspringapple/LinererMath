P132 - 148   9.14
![[0b784c4a-1f8a-42f0-8035-7dd2da16d6c2.png]]
### 0. 二次方程

$ax^2 + bxy + cy^2 + dx + ey + c = 0$

- 常数项（c） **不会改变函数图像的形状**，只会变化大小
- 一次项（ 例如 $dx, ey$ ) 对图像做了**平移，也不改变形状**
- 二次项  决定了图像的形状，例如 $x^2 + y^2 = 0$ 是个（椭）圆形， $2x^2 - 3y^2 = 0$ 是个双曲线等

**只含有二次项的函数 ==> 二次型**

二次函数可以用矩阵来表示： $\vec{x}^T \cdot A \cdot \vec{x}$

二次函数写成矩阵的方法：

- 平方项 - 对角线
- 交叉项 - 除以2后填充相应非对角位置

![[8385e61e-9b08-4bee-9bdf-3c4188f6d443.png]]


拆分交叉项时候 - 把系数拆为  2b 形式

**对称矩阵好处** ：特征值是实数，图形上只有拉伸，没有旋转（虚特征值会旋转）

![[66c5fbc9-95d5-43a5-a987-57109e3ec85d.png]]

研究二次函数 => 研究二次矩阵

$f = \vec{x}^TA\vec{x}$ 的**拆分理解**：
- $A\vec{x}$ - 向量线性变换
- $\vec{x}^T \cdot A\vec{x}$ - 点积
- 点积结果：**正数** - 同侧  **负数** - 非同侧  **零** - 垂直正交

二次型结果也就是上面点积结果，**正数** - 正定  **负数** - 负定   **不确定符号** - 不定    

**大于等于零** - 半正定   **小于等于零** - 半负定

**正定矩阵** ---> 有最小值
**负定矩阵** ---> 有最大值

**正定判定**：特征是是否都是正数（充分条件）

顺序主子式：



### 1. 二次型

- 线性函数 x ↦ Ax，它们是一维情况下直线 f(x) = ax（R → R）在**高维的推广**
- **二次型**则是将二次函数 $f(x) = ax^2 = xax$ **推广到高维**

**定义 6.1.1**：$R^n$ 上的 **二次型** 是一个函数 $Q: R^n → R$，由 $Q(x) = x^T Ax$ 给出，其中 $A \in R^{n \times n}$ 是**对称矩阵**

二次型在**函数的最大化或最小化中**有重要应用。例如，我们回忆一下第5.8.3节中的线性模型。在那里，我们试图找到一个能够最小化误差平方和的参数。

对此的一个推广原理——**最大似然方法**，**可以毫不夸张地被视为现代统计学的核心**，将在下学期详细讨论。在这个方法中，我们通过最大化所谓的似然函数来调整统计模型的参数，即使在训练具有万亿参数的现代神经网络时，也会用到这个原理。

这种联系可以通过**泰勒定理**来建立：设 f : Rⁿ → R 是一个充分光滑的函数，x* 是使该函数最小的点。那么在 x* 的邻域内，我们可以将函数表示为：

![[bcc6c5b6-e784-4cfd-91e5-3537bab719a3.png]]

- ∇f(x*) 是梯度
- H(x*) = ∇²f(x*) 是在点 x* 处的黑塞矩阵
- f(x*) 是常数，并且由于 f 在 x* 处有最小值
- 因此函数 f 可以局部地通过二次型 $Q(z) = z^T H(x*)z$ 来近似

举例：**求函数最小值**

**微积分视角：**

- $f(x,y) = x^2 +4y^2$
- 对 $x$ 和 $y$ 求导：
	- 对 $x$ 求导： $2x = 0 \to x = 0$
	- 对 $y$ 求导： $8y = 0 \to y = 0$
	- 二阶导数都是正数，所以 $(0,0)$ 有最小值

**线性代数视角：**

- 函数改写：$f(x,y) = \begin{bmatrix} x & y \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}$
- 看矩阵：$\begin{bmatrix} 1 & 0 \\ 0 & 4 \end{bmatrix}$
- 特征值是 1  和 4
- 所以这是"**正定矩阵**"（特征值都是正的）
- 结论：**一定有最小值**！


- **普通二次型**：$f(x,y,z) = x^2 + 4y^2 + 3z^2 + 2yz$ 
- **标准型**：只有平方项
- **规范型**：只有平方项，且平方项系数只能是-1，1，0
- **正定型**：平方系数全部为正
- **非正定**：平方系数不全为正

#### 1.1 主轴定理

一个二次型：![[6c67aac3-8eeb-4672-bf6a-ab19cc0f4730.png]]

既包含：
- 纯二次项 aᵢᵢxᵢ²
- 也包含混合项 aᵢⱼxᵢxⱼ
- 它可以**正交对角化**为 $A = PΛP^T$
- 其中 P = (p₁ ⋯ pₙ) 的列是正交特征向量
- 因此有 $x^T Ax = x^T PΛP^T x = (P^T x)^T Λ(P^T x)$


**正交对角化**：- $A = PDP^T$

通过坐标变换到**特征向量基**  $y = P^T x$，**我们可以将二次型表示为纯二次项**

**定理**：$A \in ℝ^{n \times n}$ 是一个**对称矩阵**，通过 P **正交地对角化**为矩阵 $Λ = diag(λ₁,...,λₙ)$，那么对于 $y = P^T x$，有：$x^T Ax = y^T Λy = \Sigma_{i=1}^n \lambda_i \cdot y_i^2$



- 因为：$Q(x) = x^TAx = x^TPDP^Tx = (P^Tx)^T \cdot D \cdot P^Tx = y^TDy, y = P^Tx$
- 所以：$Q(x) = \lambda_1y_1^2 + ... + \lambda_ny_n^2$
- 所有：**任何二次型都是若干个独立方向上的抛物线的叠加!!**

- $y = P^Tx$ 是坐标 $x$ 的坐标变换
- $x = [x_1, x_2,..., x_n]^T$ 是原始坐标
- $y = [y_1, y_2, ..., y_n]^T$ 是新坐标（主轴坐标）
- $y_i$ 是向量 y 的第 i 个分量

**任何二次型都是若干个独立方向上的抛物线的叠加!!**

#### 1.2 正定性

可以通过正定性来判断一个临界点是极大值、极小值还是鞍点

**定义**：设 A ∈ ℝⁿˣⁿ 是对称矩阵，$Q(x) = x^T Ax$ 是相应的二次型。那么我们称 A 和 Q 为：

• **正定的**，如果对所有 x ≠ 0，都有 Q(x) > 0； -- 抛物线开口朝上 ， **有全局极小值**

• **半正定的**，如果对所有 x ≠ 0，都有 Q(x) ≥ 0； -- 开口朝上并有0点，**有极小值，但不是严格极小值**

• **负定的**，如果对所有 x ≠ 0，都有 Q(x) < 0； -- 抛物线开口朝下，**并且具有全局极大值**

• **半负定的**，如果对所有 x ≠ 0，都有 Q(x) ≤ 0；-- 开口朝下并有零点

• **不定的**，如果既存在 x ∈ ℝⁿ 使得 Q(x) > 0，也存在另一个 x ∈ ℝⁿ 使得 Q(x) < 0。 -- 有的取值开口朝上，有的朝下，**既没有极小值也没有极大值**


**定理**：

设 A ∈ ℝⁿˣⁿ 是对称矩阵。那么二次型 Q(x) = x^T Ax 满足：

(i) **正定**，当且仅当所有特征值都严格为正；

(ii) **半正定**，当且仅当所有特征值都非负；

(iii) **负定**，当且仅当所有特征值都严格为负；

(iv) **半负定**，当且仅当所有特征值都非正；

(v) **不定**，当且仅当存在一个严格为正的特征值和一个严格为负的特征值。


$Q(x) = x^T Ax = y^T Λy = \Sigma_{i=1}^n λ_iy_i^2$

其中 $y = P^T x$，而 λ₁, ..., λₙ 是 A 的特征值。由于 P 是可逆的，对每个 x 恰好存在唯一的 y，使得 $y = P^T x$。因此，当 x ≠ 0 时，Q(x) 取到的值恰好与 Σᵢ₌₁ⁿ λᵢyᵢ² 相同。这个和式的符号显然可以通过定理中提到的各种情况来刻画。


#### 1.3 特征值的变分刻画

"变分刻画"（variationelle Charakterisierung）是指通过**优化问题（求极值）**来描述特征值的方法。这种方法的核心思想是：

- 特征值可以通过求解约束优化问题得到
- 具体来说，是在某些约束条件下（如 ||x|| = 1）最大化或最小化二次型 $x^T Ax$
- 这提供了一种几何直观的理解特征值的方式

...


#### 1.4 奇异值分解（SVD）


特征值的变分表述虽然只适用于对称矩阵，但现在将证明它在更广泛的范围内也很有用。我们已经充分探讨了二次矩阵对角化的用处和认识价值。特别优美的是**通过正交矩阵进行对角化**，遗憾的是这只对**对称矩阵**可行。我们自然也希望**对任意的 (m × n) 矩阵**有类似的东西。

事实上这是可能的：有多种方式将矩阵 A ∈ ℝᵐˣⁿ 分解为 A = PDQ⁻¹，其中 D 是"对角"的，P 和 Q 是可逆的。其中最有用的可能是所谓的奇异值分解（英文：singular value decomposition/SVD）。在这种分解中，P 和 Q 都是正交矩阵。

其思想是模仿特征值分解的一个重要解释。如果 A 是对称的，λ 是绝对值最大的特征值，那么相应的特征向量给出了变换 A 拉伸最强的方向。绝对值第二大的特征值对应的特征向量给出了一个与第一个向量正交的方向，在这个方向上 A 拉伸第二强。绝对值最小的特征值对应的特征向量给出了 A 压缩最强的方向。如果 A ∈ ℝᵐˣⁿ 不是方阵，我们现在也可以寻找这样的方向，只是不再得到特征向量了。

这在图 6.3 中有所说明，左边是单位球面，右边是经过 A ∈ ℝ²ˣ³ 变换后的像。在球面上，所有向量 x ∈ ℝ³ 都有相同的长度。但经过变换后，有些向量的长度比其他的更大。长度最大的向量是 y = (18, 6)，长度最小的是 y = (3, -9)。我们可以用两种方式描述这些"方向"：作为满足 Ax = y 的 x ∈ ℝ³，或者通过 y ∈ ℝ² 本身。这两个视角就给出了分解中的矩阵 P 和 Q。我们将在下面将这个想法形式化。









