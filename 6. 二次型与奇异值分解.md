P132 - 148   9.14

### 1. 二次型

- 线性函数 x ↦ Ax，它们是一维情况下直线 f(x) = ax（R → R）在**高维的推广**
- **二次型**则是将二次函数 $f(x) = ax^2 = xax$ **推广到高维**

**定义 6.1.1**：$R^n$ 上的 **二次型** 是一个函数 $Q: R^n → R$，由 $Q(x) = x^T Ax$ 给出，其中 $A \in R^{n \times n}$ 是**对称矩阵**

二次型在**函数的最大化或最小化中**有重要应用。例如，我们回忆一下第5.8.3节中的线性模型。在那里，我们试图找到一个能够最小化误差平方和的参数。

对此的一个推广原理——**最大似然方法**，**可以毫不夸张地被视为现代统计学的核心**，将在下学期详细讨论。在这个方法中，我们通过最大化所谓的似然函数来调整统计模型的参数，即使在训练具有万亿参数的现代神经网络时，也会用到这个原理。

这种联系可以通过**泰勒定理**来建立：设 f : Rⁿ → R 是一个充分光滑的函数，x* 是使该函数最小的点。那么在 x* 的邻域内，我们可以将函数表示为：

![[bcc6c5b6-e784-4cfd-91e5-3537bab719a3.png]]

- ∇f(x*) 是梯度
- H(x*) = ∇²f(x*) 是在点 x* 处的黑塞矩阵
- f(x*) 是常数，并且由于 f 在 x* 处有最小值
- 因此函数 f 可以局部地通过二次型 $Q(z) = z^T H(x*)z$ 来近似

举例：**求函数最小值**

**微积分视角：**

- $f(x,y) = x^2 +4y^2$
- 对 $x$ 和 $y$ 求导：
	- 对 $x$ 求导： $2x = 0 \to x = 0$
	- 对 $y$ 求导： $8y = 0 \to y = 0$
	- 二阶导数都是正数，所以 $(0,0)$ 有最小值

**线性代数视角：**

- 函数改写：$f(x,y) = \begin{bmatrix} x & y \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}$
- 看矩阵：$\begin{bmatrix} 1 & 0 \\ 0 & 4 \end{bmatrix}$
- 特征值是 1  和 4
- 所以这是"**正定矩阵**"（特征值都是正的）
- 结论：**一定有最小值**！


- **普通二次型**：$f(x,y,z) = x^2 + 4y^2 + 3z^2 + 2yz$ 
- **标准型**：只有平方项
- **规范型**：只有平方项，且平方项系数只能是-1，1，0
- **正定型**：平方系数全部为正
- **非正定**：平方系数不全为正

#### 1.1 主轴定理

一个二次型：![[6c67aac3-8eeb-4672-bf6a-ab19cc0f4730.png]]

既包含：
- 纯二次项 aᵢᵢxᵢ²
- 也包含混合项 aᵢⱼxᵢxⱼ
- 它可以**正交对角化**为 $A = PΛP^T$
- 其中 P = (p₁ ⋯ pₙ) 的列是正交特征向量
- 因此有 $x^T Ax = x^T PΛP^T x = (P^T x)^T Λ(P^T x)$

通过坐标变换到**特征向量基**  $y = P^T x$，**我们可以将二次型表示为纯二次项**

**定理**：$A \in ℝ^{n \times n}$ 是一个**对称矩阵**，通过 P 正交地对角化为矩阵 $Λ = diag(λ₁,...,λₙ)$，那么对于 $y = P^T x$，有：$x^T Ax = y^T Λy = \Sigma_{i=1}^n \lambda_i \cdot y_i^2$

- $y = P^Tx$ 是坐标 $x$ 的坐标变换
- $x = [x_1, x_2,..., x_n]^T$ 是原始坐标
- $y = [y_1, y_2, ..., y_n]^T$ 是新坐标（主轴坐标）
- $y_i$ 是向量 y 的第 i 个分量

**任何二次型都是若干个独立方向上的抛物线的叠加!!**

