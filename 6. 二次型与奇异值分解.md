P132 - 148   9.14
![[0b784c4a-1f8a-42f0-8035-7dd2da16d6c2.png]]
### 0. 二次方程

$ax^2 + bxy + cy^2 + dx + ey + c = 0$

- 常数项（c） **不会改变函数图像的形状**，只会变化大小
- 一次项（ 例如 $dx, ey$ ) 对图像做了**平移，也不改变形状**
- 二次项  决定了图像的形状，例如 $x^2 + y^2 = 0$ 是个（椭）圆形， $2x^2 - 3y^2 = 0$ 是个双曲线等

**只含有二次项的函数 ==> 二次型**

二次函数可以用矩阵来表示： $\vec{x}^T \cdot A \cdot \vec{x}$

二次函数写成矩阵的方法：

- 平方项 - 对角线
- 交叉项 - 除以2后填充相应非对角位置

![[8385e61e-9b08-4bee-9bdf-3c4188f6d443.png]]


拆分交叉项时候 - 把系数拆为  2b 形式

**对称矩阵好处** ：特征值是实数，图形上只有拉伸，没有旋转（虚特征值会旋转）

![[66c5fbc9-95d5-43a5-a987-57109e3ec85d.png]]

研究二次函数 => 研究二次矩阵

$f = \vec{x}^TA\vec{x}$ 的**拆分理解**：
- $A\vec{x}$ - 向量线性变换
- $\vec{x}^T \cdot A\vec{x}$ - 点积
- 点积结果：**正数** - 同侧  **负数** - 非同侧  **零** - 垂直正交

二次型结果也就是上面点积结果，**正数** - 正定  **负数** - 负定   **不确定符号** - 不定    

**大于等于零** - 半正定   **小于等于零** - 半负定

**正定矩阵** ---> 有最小值
**负定矩阵** ---> 有最大值

**正定判定**：特征是是否都是正数（充分条件）

顺序主子式：



### 1. 二次型

- 线性函数 x ↦ Ax，它们是一维情况下直线 f(x) = ax（R → R）在**高维的推广**
- **二次型**则是将二次函数 $f(x) = ax^2 = xax$ **推广到高维**

**定义 6.1.1**：$R^n$ 上的 **二次型** 是一个函数 $Q: R^n → R$，由 $Q(x) = x^T Ax$ 给出，其中 $A \in R^{n \times n}$ 是**对称矩阵**

二次型在**函数的最大化或最小化中**有重要应用。例如，我们回忆一下第5.8.3节中的线性模型。在那里，我们试图找到一个能够最小化误差平方和的参数。

对此的一个推广原理——**最大似然方法**，**可以毫不夸张地被视为现代统计学的核心**，将在下学期详细讨论。在这个方法中，我们通过最大化所谓的似然函数来调整统计模型的参数，即使在训练具有万亿参数的现代神经网络时，也会用到这个原理。

这种联系可以通过**泰勒定理**来建立：设 f : Rⁿ → R 是一个充分光滑的函数，x* 是使该函数最小的点。那么在 x* 的邻域内，我们可以将函数表示为：

![[bcc6c5b6-e784-4cfd-91e5-3537bab719a3.png]]

- ∇f(x*) 是梯度
- H(x*) = ∇²f(x*) 是在点 x* 处的黑塞矩阵
- f(x*) 是常数，并且由于 f 在 x* 处有最小值
- 因此函数 f 可以局部地通过二次型 $Q(z) = z^T H(x*)z$ 来近似

举例：**求函数最小值**

**微积分视角：**

- $f(x,y) = x^2 +4y^2$
- 对 $x$ 和 $y$ 求导：
	- 对 $x$ 求导： $2x = 0 \to x = 0$
	- 对 $y$ 求导： $8y = 0 \to y = 0$
	- 二阶导数都是正数，所以 $(0,0)$ 有最小值

**线性代数视角：**

- 函数改写：$f(x,y) = \begin{bmatrix} x & y \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}$
- 看矩阵：$\begin{bmatrix} 1 & 0 \\ 0 & 4 \end{bmatrix}$
- 特征值是 1  和 4
- 所以这是"**正定矩阵**"（特征值都是正的）
- 结论：**一定有最小值**！


- **普通二次型**：$f(x,y,z) = x^2 + 4y^2 + 3z^2 + 2yz$ 
- **标准型**：只有平方项
- **规范型**：只有平方项，且平方项系数只能是-1，1，0
- **正定型**：平方系数全部为正
- **非正定**：平方系数不全为正

#### 1.1 主轴定理

一个二次型：![[6c67aac3-8eeb-4672-bf6a-ab19cc0f4730.png]]

既包含：
- 纯二次项 aᵢᵢxᵢ²
- 也包含混合项 aᵢⱼxᵢxⱼ
- 它可以**正交对角化**为 $A = PΛP^T$
- 其中 P = (p₁ ⋯ pₙ) 的列是正交特征向量
- 因此有 $x^T Ax = x^T PΛP^T x = (P^T x)^T Λ(P^T x)$


**正交对角化**：- $A = PDP^T$

通过坐标变换到**特征向量基**  $y = P^T x$，**我们可以将二次型表示为纯二次项**

**定理**：$A \in ℝ^{n \times n}$ 是一个**对称矩阵**，通过 P **正交地对角化**为矩阵 $Λ = diag(λ₁,...,λₙ)$，那么对于 $y = P^T x$，有：$x^T Ax = y^T Λy = \Sigma_{i=1}^n \lambda_i \cdot y_i^2$



- 因为：$Q(x) = x^TAx = x^TPDP^Tx = (P^Tx)^T \cdot D \cdot P^Tx = y^TDy, y = P^Tx$
- 所以：$Q(x) = \lambda_1y_1^2 + ... + \lambda_ny_n^2$
- 所有：**任何二次型都是若干个独立方向上的抛物线的叠加!!**

- $y = P^Tx$ 是坐标 $x$ 的坐标变换
- $x = [x_1, x_2,..., x_n]^T$ 是原始坐标
- $y = [y_1, y_2, ..., y_n]^T$ 是新坐标（主轴坐标）
- $y_i$ 是向量 y 的第 i 个分量

**任何二次型都是若干个独立方向上的抛物线的叠加!!**

#### 1.2 正定性

可以通过正定性来判断一个临界点是极大值、极小值还是鞍点

**定义**：设 A ∈ ℝⁿˣⁿ 是对称矩阵，$Q(x) = x^T Ax$ 是相应的二次型。那么我们称 A 和 Q 为：

• **正定的**，如果对所有 x ≠ 0，都有 Q(x) > 0； -- 抛物线开口朝上 ， **有全局极小值**

• **半正定的**，如果对所有 x ≠ 0，都有 Q(x) ≥ 0； -- 开口朝上并有0点，**有极小值，但不是严格极小值**

• **负定的**，如果对所有 x ≠ 0，都有 Q(x) < 0； -- 抛物线开口朝下，**并且具有全局极大值**

• **半负定的**，如果对所有 x ≠ 0，都有 Q(x) ≤ 0；-- 开口朝下并有零点

• **不定的**，如果既存在 x ∈ ℝⁿ 使得 Q(x) > 0，也存在另一个 x ∈ ℝⁿ 使得 Q(x) < 0。 -- 有的取值开口朝上，有的朝下，**既没有极小值也没有极大值**


**定理**：

设 A ∈ ℝⁿˣⁿ 是对称矩阵。那么二次型 Q(x) = x^T Ax 满足：

(i) **正定**，当且仅当所有特征值都严格为正；

(ii) **半正定**，当且仅当所有特征值都非负；

(iii) **负定**，当且仅当所有特征值都严格为负；

(iv) **半负定**，当且仅当所有特征值都非正；

(v) **不定**，当且仅当存在一个严格为正的特征值和一个严格为负的特征值。


$Q(x) = x^T Ax = y^T Λy = \Sigma_{i=1}^n λ_iy_i^2$

其中 $y = P^T x$，而 λ₁, ..., λₙ 是 A 的特征值。由于 P 是可逆的，对每个 x 恰好存在唯一的 y，使得 $y = P^T x$。因此，当 x ≠ 0 时，Q(x) 取到的值恰好与 Σᵢ₌₁ⁿ λᵢyᵢ² 相同。这个和式的符号显然可以通过定理中提到的各种情况来刻画。


#### 1.3 特征值的变分刻画

"变分刻画"（variationelle Charakterisierung）是指通过**优化问题（求极值）**来描述特征值的方法。这种方法的核心思想是：

- 特征值可以通过求解约束优化问题得到
- 具体来说，是在某些约束条件下（如 ||x|| = 1）最大化或最小化二次型 $x^T Ax$
- 这提供了一种几何直观的理解特征值的方式

...
想象你有一个对称矩阵A，我们想找它的特征值。变分刻画告诉我们：**特征值实际上是某些优化问题的答案**。

## 核心思想

对于一个n×n的对称矩阵A，它的特征值可以这样理解：

**最大特征值**：在所有长度为1的向量中，找一个向量x，使得x^T A x（二次型）达到最大值。这个最大值就是最大特征值，而达到最大值的那个向量就是对应的特征向量。

用数学语言写就是：λ₁ = max{x^T A x : ||x||=1}

## 瑞利商

这里有个重要概念叫瑞利商（Rayleigh quotient）：R(x) = (x^T A x)/(x^T x)

瑞利商的含义很直观：它衡量了向量x经过矩阵A变换后，"能量"放大了多少倍。变分刻画告诉我们，瑞利商的最大值就是最大特征值，最小值就是最小特征值。

## 一个直观的例子

想象一个椭圆，它的方程是x^T A x = 1。椭圆的最长轴方向就对应最大特征值的特征向量，轴的长度与特征值成反比。最短轴对应最小特征值。这样，找特征值就变成了找椭圆主轴的问题，这就是一个几何上的优化问题。

**总之，变分刻画把"解方程"的代数问题转化为"找最值"的优化问题，让我们从另一个角度理解特征值的本质。**


**定理**：设 $A \in \mathbb{R}^{n \times n}$ 是对称矩阵，其特征值按从大到小的方式排列为 $\lambda_1 \geq \lambda_2 \geq ...\geq \lambda_n$，则有：

		$\lambda_n = min_{||x|| = 1}x^TAx \leq max_{||x|| = 1}x^TAx = \lambda_1$

- 可以理解为 函数 $f(x_1,...,x_n)$ 在 $||\{x_1,...,x_n\}|| = 1$ 的时候，函数值最小，最小值就是最小的特征值，最大值就是最大的特征值

你可以把 f(x) = x^T A x 看作一个定义在 ℝⁿ 上的函数，其中 x = (x₁, x₂, ..., xₙ)^T。这个定理告诉我们：

**在所有单位球面上的点**（即满足 ||x|| = 1 的所有向量），这个函数 f(x) = x^T A x：

- 取到最大值时，最大值恰好等于最大特征值 λ₁
- 取到最小值时，最小值恰好等于最小特征值 λₙ

![[75840a7b-905e-4044-bcdc-34d6fbfe608a.png]]

$Q(x) = 3x_1^2 + 7x_2^2$

- 给定 $x_1,x_2$，如果满足 $||\{x_1,x_2\}|| = 1$，即图形下面坐标轴上的圆型的边缘，对应的函数值也就是右侧图形的红色值。
- 函数值最大的部分就是7，最小的是3，是二次型的特征值3和7

![[cd144977-04dd-4f90-b312-26a7b0e22473.png]]

对二次型进行正交变换后，不改变向量长度，因此 $||y|| = ||x|| = 1$


**定理**：**Courant-Fischer 极小极大定理**（Min-Max Theorem）

![[5359f32d-0db0-4325-b0fe-94e8726adf4b.png]]

#### 1.4 奇异值分解（SVD）


特征值的变分表述虽然只适用于对称矩阵，但现在将证明它在更广泛的范围内也很有用。我们已经充分探讨了二次矩阵对角化的用处和认识价值。特别优美的是**通过正交矩阵进行对角化**，遗憾的是这只对**对称矩阵**可行。我们自然也希望**对任意的 (m × n) 矩阵**有类似的东西。

事实上这是可能的：

- 有多种方式将矩阵 A ∈ ℝᵐˣⁿ 分解为 A = PDQ⁻¹，其中 D 是"对角"的，P 和 Q 是可逆的。其中最有用的可能是所谓的奇异值分解（英文：singular value decomposition/SVD）。在这种分解中，P 和 Q 都是正交矩阵。

- 其思想是模仿特征值分解的一个重要解释。如果 A 是对称的，λ 是绝对值**最大**的特征值，那么相应的特征向量给出了变换 A **拉伸最强的方向**。绝对值第二大的特征值对应的特征向量给出了一个与第一个向量正交的方向，在这个方向上 A 拉伸第二强。**绝对值最小的特征值对应的特征向量给出了 A 压缩最强的方向**。如果 A ∈ ℝᵐˣⁿ 不是方阵，我们现在也可以寻找这样的方向，只是不再得到特征向量了。

这在图 6.3 中有所说明，左边是单位球面，右边是经过 A ∈ ℝ²ˣ³ 变换后的像。在球面上，所有向量 x ∈ ℝ³ 都有相同的长度。但经过变换后，有些向量的长度比其他的更大。长度最大的向量是 y = (18, 6)，长度最小的是 y = (3, -9)。我们可以用两种方式描述这些"方向"：作为满足 Ax = y 的 x ∈ ℝ³，或者通过 y ∈ ℝ² 本身。这两个视角就给出了分解中的矩阵 P 和 Q。我们将在下面将这个想法形式化。



### 对称矩阵的美好世界 ✨

- **特征值分解**：A = PΛP^T（P正交，Λ对角）
- **几何意义**：特征向量是"主轴"，特征值是"拉伸倍数"
- **限制**：只适用于方阵，且要对称才能正交对角化

### 现实的需求 🎯

但实际中我们经常遇到：

- **非方阵**：比如 100×50 的数据矩阵
- **非对称矩阵**：大多数矩阵都不对称

问题：能否找到类似的"漂亮"分解？

## SVD的核心思想：寻找最大拉伸方向

### 直观理解：矩阵是"变形器" 🔄

想象矩阵 A 是一个"变形器"：

1. **输入**：n维空间的单位球（所有长度为1的向量）
2. **输出**：m维空间的椭球

### 关键观察 👁️

当我们用 A 变换单位球时：

- 有些方向被**拉长**了很多
- 有些方向被**压缩**了
- 有些方向可能**介于两者之间**

### 两个视角 🔍

对于"最大拉伸"这件事，我们可以问：

1. **输入视角**：哪个输入向量 v ∈ ℝⁿ 被拉伸最多？
2. **输出视角**：输出空间中哪个方向 u ∈ ℝᵐ 对应最大拉伸？

这就给出了SVD的两组正交向量！


**原始坐标 → $V^T$ 旋转 → $Σ$ 拉伸 → U 旋转 → 新坐标**


![[a899498b-39df-431e-b173-53567f9b9b99.png]]

**将任意矩阵分解**

$M = U \cdot \Sigma \cdot V^T$

- U - 旋转
- ∑ - 拉伸 对角元素表示拉升强度
- $V^T$ - 旋转

意义：找到一组基，在线性变换后，基还是正交的（垂直）
![[98c32268-ef08-43d5-8f12-ab7489416dd9.png]]

- V - 原始的标准正交基
- U - 经过M变换后的标准正交基

正交矩阵转置就是逆

![[95874bde-5e17-485d-8f40-35376a0ac7ec.png]]

取前r个奇异值（从大到小）

特征值平方根就是奇异值 西格玛


![[ec9b93f8-606a-44a9-8961-55d3ecc6ee99.png]]

u v 是归一化的特征向量，从大到小排列

![[d07b399f-415a-4b64-94f0-9042e46ec0d9.png]]


半正定对角矩阵 - 伸缩变换
正交矩阵 - 旋转变换

![[81f09a89-58bd-440d-9ff1-99ec520e34b7.png]]


![[19cce278-87c1-4f30-a657-ec40366c8db8.png]]![[9a2bdd1f-e9c6-4099-9c19-9f023e31aaaa.png]]

实对称矩阵 -- 一定对角化

一般矩阵  AA^T = 实对称矩阵

A = PSQ^T

PQ是正交矩阵，PP^T = I QQ^T = I

AA^T = PSQ^T  x  (PSQ^T)^T = PS^2P^T
A^TA = QS^2Q^T



![[86a6a159-1b11-48fe-b2e9-ae860dfdf7cd.png]]

